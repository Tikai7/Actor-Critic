{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f291981c",
   "metadata": {},
   "source": [
    " Copyright © Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the\n",
    " LICENSE file in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587b539b",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook, you will code a naive actor-critic algorithm in the tabular case. Then you will tune it using grid search and Bayesian optimization, potentially using the [optuna](https://optuna.readthedocs.io/en/stable/) library.\n",
    "Finally, you will get the best hyper-parameters obtained with both methods and perform a statistical test to see if there is a statistically significant difference between these methods and with respect to naive hyper-parameter values.\n",
    "\n",
    "## Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec87094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installs the necessary Python and system libraries\n",
    "try:\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "except ModuleNotFoundError as e:\n",
    "    get_ipython().run_line_magic(\"pip\", \"install 'easypip>=1.2.0'\")\n",
    "    from easypip import easyimport, easyinstall, is_notebook\n",
    "\n",
    "easyinstall(\"swig\")\n",
    "easyinstall(\"bbrl>=0.2.2\")\n",
    "easyinstall(\"bbrl_gymnasium>=0.2.0\")\n",
    "easyinstall(\"tensorboard\")\n",
    "easyinstall(\"moviepy\")\n",
    "easyinstall(\"box2d-kengz\")\n",
    "easyinstall(\"optuna\")\n",
    "easyinstall(\"gymnasium\")\n",
    "easyinstall(\"mazemdp\")\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from typing import List\n",
    "\n",
    "import hydra\n",
    "import optuna\n",
    "import yaml\n",
    "from omegaconf import OmegaConf, DictConfig\n",
    "\n",
    "# For visualization\n",
    "os.environ[\"VIDEO_FPS\"] = \"5\"\n",
    "if not os.path.isdir(\"./videos\"):\n",
    "    os.mkdir(\"./videos\")\n",
    "\n",
    "from IPython.display import Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b2e8a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6281c86",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from bbrl.utils.chrono import Chrono\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mazemdp.toolbox import sample_categorical\n",
    "from mazemdp.mdp import Mdp\n",
    "from bbrl_gymnasium.envs.maze_mdp import MazeMDPEnv\n",
    "from gymnasium.wrappers.monitoring.video_recorder import VideoRecorder\n",
    "from functools import partial\n",
    "\n",
    "matplotlib.use(\"TkAgg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4073b453",
   "metadata": {},
   "source": [
    "# Step 1: Coding the naive Actor-critic algorithm\n",
    "\n",
    "We consider the naive actor-critic algorithm with a categorical policy.\n",
    "The algorithm learns a critic with the standard temporal difference mechanism\n",
    "using a learning rate $\\alpha_{critic}$.\n",
    "\n",
    "We consider a value-based critic $V(s)$. The extension to an action value function $Q(s,a)$ is straightforward.\n",
    "\n",
    "To update the critic, the algorithm computes the temporal difference error:\n",
    "\n",
    "$$\\delta_t = r(s_t, a_t) + \\gamma V^{(n)}(s_{t+1})-V^{(n)}(s_t).$$\n",
    "\n",
    "Then it applies it to the critic:\n",
    "\n",
    "$$V^{(n+1)}(s_t) = V^{(n)}(s_t) + \\alpha_{critic} \\delta_t.$$\n",
    "\n",
    "To update the actor, the general idea is the same, using the temporal difference error with another learning rate $\\alpha_{actor}$.\n",
    "\n",
    "However, naively applying the same learning rule would not ensure that the probabilities of all actions in a state sum to 1.\n",
    "Besides, when the temporal difference error $\\delta_t$ is negative, it may happen that the probability of an action gets negative or null, which raises an issue when applying renormalization.\n",
    "\n",
    "So, instead of applying the naive rule, we apply the following one:\n",
    "$$ \n",
    "\\pi_{temp}(a_t|s_t) =  \\begin{cases}\n",
    "\\pi^{(i)}(a_t|s_t) + \\alpha_{actor} \\delta_t & \\mathrm{if } \\pi^{(i)}(a_t|s_t) + \\alpha_{actor} \\delta_t > 10^{-8}\\\\\n",
    "10^{-8} & \\mathrm{otherwise.} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then we can apply renormalization so that the probabilities of actions still sum to 1, with\n",
    "$$\n",
    "\\forall a, \\pi^{(i+1)}(a|s_t) = \\frac{\\pi_{temp}^{(i+1)}(a|s_t)} {\\sum_{a'} \\pi_{temp}^{(i+1)}(a'|s_t)}\n",
    "$$ with\n",
    "$$ \n",
    "\\pi_{temp}^{(i+1)}(a|s_t) =  \\begin{cases}\n",
    "\\pi_{temp}(a|s_t) & \\mathrm{if } a = a_t\\\\\n",
    "\\pi^{(i)}(a|s_t) & \\mathrm{otherwise.} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "## Exercise 1\n",
    "\n",
    "### 1. Code the naive actor-critic algorithm as specified above.\n",
    "\n",
    "Some hints:\n",
    "\n",
    "- a good idea to build this code it to take inspiration from the code of Q-learning, to add an actor (a categorical policy), both learning rates,\n",
    "and to take care about the renormalization function.\n",
    "\n",
    "- for the next steps of this lab, having a function to repeatedly call your actor-critic algorithm and save the learning trajectories and\n",
    "norms of the value function is a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224f77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np \n",
    "\n",
    "# Environment with 20% of walls and no negative reward when hitting a wall\n",
    "env = gym.make(\n",
    "    \"MazeMDP-v0\",\n",
    "    kwargs={\"width\": 4, \"height\": 3, \"ratio\": 0.2, \"hit\": 0.0},\n",
    "    render_mode=\"human\",\n",
    ")\n",
    "env.reset()\n",
    "env.unwrapped.init_draw(\"The maze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7643de",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_policy_from_actor(actor: np.ndarray) -> np.ndarray:\n",
    "    return np.argmax(actor,axis=1)\n",
    "\n",
    "def actor_critic(\n",
    "    mdp: MazeMDPEnv,\n",
    "    nb_episodes: int = 20,\n",
    "    timeout: int = 50,\n",
    "    alpha_critic : float = 0.5,\n",
    "    alpha_actor : float=0.5,\n",
    "    render: bool = True,\n",
    ") -> tuple[np.ndarray, List[float]]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        mdp: the environment\n",
    "        nb_episodes: the number of episodes\n",
    "        timeout: the maximum number of steps per episode\n",
    "        alpha_critic: the critic learning rate\n",
    "        alpha_actor: the actor learning rate\n",
    "        render: whether to render the environment\n",
    "    Returns:\n",
    "        a_probs: the learned policy\n",
    "        v: the learned value function\n",
    "        v_list: the norm of the value function at\n",
    "        time_list: the number of steps per episode\n",
    "        mean_reward: the mean reward\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the state-action value function\n",
    "    # alpha is the learning rate\n",
    "    v = np.zeros(mdp.unwrapped.nb_states)  # initial state value v\n",
    "    a_probs = np.ones((mdp.unwrapped.nb_states, mdp.action_space.n))/mdp.action_space.n \n",
    "    v_min = np.zeros(mdp.unwrapped.nb_states)\n",
    "    v_list = []\n",
    "    time_list = []\n",
    "    reward_list = []\n",
    "    # Run learning cycle\n",
    "    mdp.timeout = timeout  # episode length\n",
    "\n",
    "    if render:\n",
    "        mdp.init_draw(\"Actor Critic\")\n",
    "\n",
    "    for _ in range(nb_episodes):\n",
    "        \n",
    "        reward = 0\n",
    "        # Draw the first state of episode i using a uniform distribution over all the states\n",
    "        x, _ = mdp.reset(uniform=True)\n",
    "        cpt = 0\n",
    "\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        while not (terminated or truncated):\n",
    "            if render:\n",
    "                mdp.draw_v_pi(v, get_policy_from_actor(a_probs))\n",
    "\n",
    "            # Draw an action using a soft-max policy\n",
    "            u = sample_categorical(a_probs[x])\n",
    "            y, r, terminated, truncated, _ = mdp.step(u)\n",
    "            # Update the state-action value function with q-Learning\n",
    "            delta = r + mdp.gamma*v[y]*(1-terminated) - v[x]\n",
    "            v[x] = v[x] + alpha_critic*delta\n",
    "\n",
    "            temp = a_probs[x,u] + alpha_actor*delta\n",
    "            a_probs[x, u] = temp if temp > 1e-8 else 1e-8\n",
    "            a_probs[x] = a_probs[x]/a_probs[x].sum()\n",
    "\n",
    "            x = y\n",
    "            cpt = cpt + 1\n",
    "            reward += r\n",
    "\n",
    "        reward_list.append(reward)\n",
    "        v_list.append(np.linalg.norm(np.maximum(v, v_min)))\n",
    "        time_list.append(cpt)\n",
    "\n",
    "    if render:\n",
    "        mdp.current_state = 0\n",
    "        mdp.draw_v_pi(v, get_policy_from_actor(a_probs))\n",
    "\n",
    "    return a_probs, v, v_list, time_list, reward_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e991705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPISODES = 100\n",
    "TIMEOUT = 50\n",
    "ALPHA_CRITIC = 0.5\n",
    "ALPHA_ACTOR = 0.5\n",
    "\n",
    "a, v, v_list, time_list, reward_list = actor_critic(\n",
    "    env, alpha_critic=ALPHA_CRITIC,alpha_actor=ALPHA_ACTOR, nb_episodes=NB_EPISODES, timeout=TIMEOUT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d21dcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 7))\n",
    "sns.heatmap(a, annot=True, fmt=\".2f\", cmap=\"viridis\")\n",
    "plt.title(\"Actor probabilities\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da53e73",
   "metadata": {},
   "source": [
    "### 2. Provide a plot function\n",
    "\n",
    "Your plot function should show the evolution through time of number of steps the agent takes to find the reward in the maze.\n",
    "If your algorithm works, this number of steps should decrease through time.\n",
    "\n",
    "Your plot function should also show a mean and a standard deviation (or some more advanced statistics) over a collection of learning runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "04f00ef3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class Plotter():\n",
    "\n",
    "    @staticmethod\n",
    "    def moving_average(data, window_size):\n",
    "        return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_steps_evolution(time_list, save_path=\"figure1\"):\n",
    "        mean = np.round(np.mean(time_list), 3)\n",
    "        std = np.round(np.std(time_list), 3)\n",
    "        q1 = np.round(np.percentile(time_list, 35),3)\n",
    "        q3 = np.round(np.percentile(time_list, 75),3)\n",
    "        median = np.round(np.median(time_list),3)\n",
    "        max = np.round(np.max(time_list),3)\n",
    "        min = np.round(np.min(time_list),3)\n",
    "\n",
    "        time_list_smoothed = Plotter.moving_average(time_list, 10)\n",
    "\n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.figure(figsize=(14, 7))\n",
    "\n",
    "        # Plot des étapes par épisode\n",
    "        plt.subplot(121)\n",
    "        plt.plot(time_list, label=\"Steps per episode\", color='blue', alpha=0.6)\n",
    "        plt.plot(time_list_smoothed, label=\"Moving average of steps per episode\", color='red', linewidth=2)\n",
    "        plt.legend()\n",
    "        plt.title(\"Number of Steps per Episode\")\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Steps\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Boxplot des étapes par épisode\n",
    "        plt.subplot(122)\n",
    "        plt.boxplot(time_list, vert=False)\n",
    "        plt.title(f\"Box plot of steps per episode\")\n",
    "        plt.xlabel(\"Steps\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        handles.extend([\n",
    "            plt.Line2D([0], [0], color='yellow', lw=2),\n",
    "            plt.Line2D([0], [0], color='yellow', lw=2),\n",
    "            plt.Line2D([0], [0], color='green', lw=2),\n",
    "            plt.Line2D([0], [0], color='purple', lw=2),\n",
    "            plt.Line2D([0], [0], color='red', lw=2),\n",
    "            plt.Line2D([0], [0], color='blue', lw=2),\n",
    "            plt.Line2D([0], [0], color='blue', lw=2)\n",
    "        ])\n",
    "        labels.extend([\n",
    "            f'Max : {max} steps', f'Min : {min} steps', \n",
    "            f'Mean : {mean} steps', f'Std : {std} steps', \n",
    "            f'Q1 : {q1} steps', f'Q3 : {q3} steps', \n",
    "            f'Median : {median} steps'\n",
    "        ])\n",
    "        \n",
    "        plt.legend(handles, labels, loc='upper right')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    @staticmethod\n",
    "    def plot_multiples(liste, labels, save_path=\"figure2\", title=\"Multiple plots\", xlabel=\"Episode\", ylabel=\"Value\"):\n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.figure(figsize=(14, 7))\n",
    "\n",
    "        colors = ['blue', 'red', 'green']\n",
    "        for i, l in enumerate(liste):\n",
    "            if save_path == \"steps_per_episode_multiple\":\n",
    "                plt.plot(Plotter.moving_average(l, 10), label=labels[i]+\" smoothed\", linewidth=4, alpha=1, color=colors[i])\n",
    "            plt.plot(l, label=labels[i], alpha=0.4, color=colors[i])\n",
    "\n",
    "        plt.legend()\n",
    "        plt.title(title)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_path)\n",
    "        plt.show()\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_plots(results:dict):\n",
    "        Plotter.plot_multiples(\n",
    "            [results[i]['lists'][\"time_list\"] for i in range(len(results))], \n",
    "            save_path=\"steps_per_episode_multiple\",\n",
    "            title=\"Steps per episode\",\n",
    "            xlabel=\"Episode\",\n",
    "            ylabel=\"Steps\",\n",
    "            labels=[results[i][\"name\"] for i in range(len(results))]\n",
    "        ) \n",
    "        Plotter.plot_multiples(\n",
    "            [results[i]['lists'][\"mean_reward\"] for i in range(len(results))], \n",
    "            save_path=\"reward_per_episode multiple\",\n",
    "            title=\"Reward per episode\",\n",
    "            xlabel=\"Episode\",\n",
    "            ylabel=\"Reward\",\n",
    "            labels=[results[i][\"name\"] for i in range(len(results))]    \n",
    "        ) \n",
    "        Plotter.plot_multiples(\n",
    "            [results[i]['lists'][\"v_list\"] for i in range(len(results))], \n",
    "            save_path=\"value_per_episode_multiple\",\n",
    "            title=\"Norm of value per episode\",\n",
    "            xlabel=\"Episode\",\n",
    "            ylabel=\"Norm of value\",\n",
    "            labels=[results[i][\"name\"] for i in range(len(results))]\n",
    "        ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f80f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotter.plot_steps_evolution(time_list, \"steps_per_episode\")\n",
    "Plotter.plot_steps_evolution(reward_list, \"reward_per_episode\")\n",
    "Plotter.plot_steps_evolution(v_list, \"value_function_per_episode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c4acf6",
   "metadata": {},
   "source": [
    "## Actor-critic hyper-parameters\n",
    "\n",
    "To represent the hyper-parameters of the experiments performed in this notebook, we suggest using the dictionary below.\n",
    "This dictionary can be read using omegaconf.\n",
    "Using it is not mandatory.\n",
    "You can also change the value of hyper-parameters or environment parameters at will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a685f967",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "ac_params = {\n",
    "    \"save_curves\": False,\n",
    "    \"save_heatmap\": True,\n",
    "    \"mdp\": {\n",
    "        \"name\": \"MazeMDP-v0\",\n",
    "        \"width\": 5,\n",
    "        \"height\": 5,\n",
    "        \"ratio\": 0.2,\n",
    "        \"render_mode\": \"rgb_array\",\n",
    "    },\n",
    "        \n",
    "    \"log_dir\": \"./tmp\",\n",
    "    \"video_dir\": \"./tmp/videos\",\n",
    "\n",
    "    \"nb_episodes\": 100,\n",
    "    \"timeout\": 200,\n",
    "    \"render\": False, # True, # \n",
    "    \"nb_repeats\": 5,\n",
    "\n",
    "    \"alpha_critic\": 0.5,\n",
    "    \"alpha_actor\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370af64a",
   "metadata": {},
   "source": [
    "### 3. Test your code\n",
    "\n",
    "Once everything looks OK, save the obtained plot for your lab report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316bb3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = OmegaConf.create(ac_params)\n",
    "\n",
    "env_config = gym.make(\n",
    "    config.mdp.name,\n",
    "    kwargs={\n",
    "        \"width\": config.mdp.width,\n",
    "        \"height\": config.mdp.height,\n",
    "        \"ratio\": config.mdp.ratio,\n",
    "        \"hit\": 0.0,\n",
    "    },\n",
    "    render_mode=config.mdp.render_mode,\n",
    ")\n",
    "\n",
    "a_probs, v, v_list, time_list, reward_list = actor_critic(\n",
    "    env_config, alpha_critic=config.alpha_critic, \n",
    "    alpha_actor=config.alpha_actor, nb_episodes=config.nb_episodes, \n",
    "    timeout=config.timeout, render=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea6dc43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotter.plot_steps_evolution(time_list, save_path=\"steps_per_episode_config_0.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932ad6ba",
   "metadata": {},
   "source": [
    "# Step 2: Tuning hyper-parameters\n",
    "\n",
    "In this part, you have to optimize two hyper-parameters of the actor-critic algorithm, namely the actor and critic learning rates.\n",
    "You have to do so using a simple grid search method and some Bayesian optimization method.\n",
    "For the latter, we suggest using the default sampler from [optuna](https://optuna.readthedocs.io/en/stable/).\n",
    "Follow the above link to understand how optuna works.\n",
    "Note that it also supports grid search and many other hyper-parameters tuning algorithms.\n",
    "\n",
    "You should make sure that the hyper-parameters tuning algorithms that you compare benefit from the same training budget\n",
    "We suggest 400 training runs overall for each method,\n",
    "which means 20 values each for the actor and the critic learning rates in the case of grid search.\n",
    "\n",
    "## Exercise 2\n",
    "\n",
    "### 1. Perform hyper-parameters tuning with two algorithms as suggested above.\n",
    "\n",
    "### 2. Provide a \"heatmap\" of the norm of the value function given the hyper-parameters, after training for each pair of hyper-parameters.\n",
    "\n",
    "### 3. Collect the value of the best hyper-parameters found with each algorithm. You will need them for Step 3.\n",
    "\n",
    "### 4. Include in your report the heatmaps and the best hyper-parameters found for each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "251a2c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = []\n",
    "\n",
    "class EnvManager():\n",
    "    @staticmethod\n",
    "    def generate_envs(nb_env):\n",
    "        envs = [\n",
    "            gym.make(\n",
    "            config.mdp.name,\n",
    "            kwargs={\n",
    "                \"width\": config.mdp.width,\n",
    "                \"height\": config.mdp.height,\n",
    "                \"ratio\": config.mdp.ratio,\n",
    "                \"hit\": 0.0,\n",
    "            },\n",
    "            render_mode=config.mdp.render_mode,\n",
    "            ) for _ in range(nb_env)         \n",
    "        ]\n",
    "        return envs\n",
    "    \n",
    "envs = EnvManager.generate_envs(5)\n",
    "\n",
    "class ObjectiveFunction():\n",
    "    \"\"\"\n",
    "    Class to define the objective function to optimize\n",
    "    Methods:\n",
    "        - objective_by\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def objective_by(trial : optuna.Trial, goal: str = \"steps\", n_iter: int = 15):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            trial: the trial\n",
    "            goal: the goal to optimize, must be one of {\"mean_steps\", \"policy_entropy\", \"value_stability\", \"mean_reward\"}\n",
    "        Returns:\n",
    "            the value of the objective function\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "        global stats\n",
    "\n",
    "        all_goals = {\"mean_steps\", \"policy_entropy\", \"value_stability\", \"mean_reward\"}\n",
    "\n",
    "        if goal not in all_goals:\n",
    "            raise ValueError(f\"Unknown goal: {goal}, must be one of {all_goals}\")\n",
    "        \n",
    "        alpha_critic = trial.suggest_float(\"alpha_critic\", 1e-2, 1.0)\n",
    "        alpha_actor = trial.suggest_float(\"alpha_actor\", 1e-2, 1.0)\n",
    "        \n",
    "\n",
    "        env_a, env_v, env_v_list, env_time_list, env_mean_reward = [], [], [], [], []\n",
    "        for env_i in envs:\n",
    "            all_a, all_v, all_v_list, all_time_list, all_mean_reward = [], [], [], [], []             \n",
    "            for _ in range(n_iter):\n",
    "                a_probs, v, v_list, time_list, reward_list = actor_critic(\n",
    "                    env_i, alpha_critic=alpha_critic, \n",
    "                    alpha_actor=alpha_actor, nb_episodes=config.nb_episodes, \n",
    "                    timeout=config.timeout, render=False\n",
    "                )\n",
    "\n",
    "                mean_reward = np.mean(reward_list)\n",
    "                all_a.append(a_probs)\n",
    "                all_v.append(v)\n",
    "                all_v_list.append(v_list)\n",
    "                all_time_list.append(np.mean(time_list))\n",
    "                all_mean_reward.append(mean_reward)\n",
    "\n",
    "            env_a.append(all_a)\n",
    "            env_v.append(all_v)\n",
    "            env_v_list.append(all_v_list)\n",
    "            env_time_list.append(all_time_list)\n",
    "            env_mean_reward.append(all_mean_reward)\n",
    "        \n",
    "        all_a = np.mean(env_a, axis=0)\n",
    "        all_v = np.mean(env_v, axis=0)\n",
    "        all_v_list = np.mean(env_v_list, axis=0)\n",
    "        all_time_list = np.mean(env_time_list, axis=0)\n",
    "        all_mean_reward = np.mean(env_mean_reward, axis=0)\n",
    "        \n",
    "\n",
    "        time_list_mean = np.mean(all_time_list)\n",
    "        v_list_mean = np.mean(all_v_list, axis=0)\n",
    "        a_probs_mean = np.mean(all_a, axis=0)\n",
    "        v_mean = np.mean(all_v, axis=0)\n",
    "        mean_reward = np.mean(all_mean_reward)\n",
    "\n",
    "        stats.append({\n",
    "            \"params\" : {\n",
    "                \"alpha_critic\": alpha_critic,\n",
    "                \"alpha_actor\": alpha_actor,\n",
    "            },\n",
    "            \"lists\" : {\n",
    "                \"time_list\": all_time_list,\n",
    "                \"v_list\": all_v_list,\n",
    "                \"a_probs\": all_a,\n",
    "                \"v\": all_v,\n",
    "                \"rewards\": all_mean_reward,\n",
    "            },\n",
    "            \"evaluation_metrics\": {\n",
    "                \"mean_steps\": -time_list_mean,\n",
    "                \"policy_entropy\": -np.sum(a_probs_mean*np.log(a_probs_mean)),\n",
    "                \"value_stability\": -np.std(v_list_mean),\n",
    "                \"mean_reward\": mean_reward,\n",
    "            },\n",
    "        })\n",
    "\n",
    "        if goal == \"mean_steps\":\n",
    "            return -time_list_mean\n",
    "        elif goal == \"policy_entropy\":\n",
    "            return -np.sum(a_probs_mean*np.log(a_probs_mean))\n",
    "        elif goal == \"value_stability\":\n",
    "            return -np.std(v_list_mean)\n",
    "        elif goal == \"mean_reward\":\n",
    "            return mean_reward\n",
    "        elif goal == \"norm_v\":\n",
    "            return np.max(v_list_mean)\n",
    "\n",
    "class TuningAlgorithm():\n",
    "    \"\"\"\n",
    "    Class to tune the hyperparameters of the actor-critic algorithm\n",
    "    Methods:\n",
    "        - tune_by_bayesian_optimization \n",
    "        - tune_by_grid_search\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def tune_by_bayesian_optimization(objective = ObjectiveFunction.objective_by, goal: str = \"mean_steps\"):\n",
    "        \"\"\" \n",
    "        Tune the hyperparameters of the actor-critic algorithm using Bayesian optimization\n",
    "        Args:\n",
    "            objective: the objective function to optimize\n",
    "        \"\"\"\n",
    "        study = optuna.create_study(direction=\"maximize\")\n",
    "        objective = partial(objective, goal=goal, n_iter=15)\n",
    "        study.optimize(objective, n_trials=400)\n",
    "        \n",
    "        print(f\"Best parameters: {study.best_params}\")\n",
    "        print(f\"Best score: {study.best_value}\")\n",
    "        study_results = study.trials_dataframe()\n",
    "        study_results.to_csv(\"study_results_bayesian_opt.csv\")\n",
    "        return study.best_params, study \n",
    "    \n",
    "    @staticmethod\n",
    "    def tune_by_grid_search(objective = ObjectiveFunction.objective_by, goal: str = \"mean_steps\"):\n",
    "        \"\"\"\n",
    "        Tune the hyperparameters of the actor-critic algorithm using grid search\n",
    "        Args:\n",
    "            objective: the objective function to optimize\n",
    "        \"\"\"\n",
    "        search_space = {\n",
    "            \"alpha_critic\": np.linspace(1e-2, 1.0, 20),\n",
    "            \"alpha_actor\": np.linspace(1e-2, 1.0, 20)\n",
    "        }\n",
    "        sampler = optuna.samplers.GridSampler(search_space)\n",
    "        study = optuna.create_study(direction=\"maximize\", sampler=sampler)\n",
    "        objective = partial(objective, goal=goal, n_iter=15)\n",
    "        study.optimize(objective)\n",
    "\n",
    "        print(f\"Best parameters: {study.best_params}\")\n",
    "        print(f\"Best score: {study.best_value}\")\n",
    "\n",
    "        study_results = study.trials_dataframe()\n",
    "        study_results.to_csv(\"study_results_grid_search.csv\")\n",
    "        return study.best_params, study "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe5be91",
   "metadata": {},
   "source": [
    "## Tune hyperparameters by grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b316a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = []\n",
    "TuningAlgorithm.tune_by_grid_search(goal=\"value_stability\")\n",
    "np.save(\"stats_grid_search.npy\", stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6d1fd8",
   "metadata": {},
   "source": [
    "## Tune hyperparameters by bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff48d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = []\n",
    "TuningAlgorithm.tune_by_bayesian_optimization(goal=\"value_stability\")\n",
    "np.save(\"stats_bayesian_optimization.npy\", stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6dc5b6",
   "metadata": {},
   "source": [
    "## Best score Grid Search : \n",
    "- ***Best parameters:*** {'alpha_critic': 0.8436842105263158, 'alpha_actor': 1.0}\n",
    "- ***Best score:*** -7.862479894535781\n",
    "## Best score Bayesian Optimization  : \n",
    "- ***Best parameters:*** {'alpha_critic': 0.804400103698065, 'alpha_actor': 0.9882472809398611}\n",
    "- ***Best score:*** -7.725128134667201"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ba521f",
   "metadata": {},
   "source": [
    "# Step 3: Statistical tests\n",
    "\n",
    "Now you have to compare the performance of the actor-critic algorithm tuned\n",
    "with all the best hyper-parameters you found before, using statistical tests.\n",
    "\n",
    "The functions below are provided to run Welch's T-test over learning curves.\n",
    "They have been adapted from a github repository: https://github.com/flowersteam/rl_stats\n",
    "You don't need to understand them in detail (though it is always a good idea to try to understand more code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2258ad0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "import bootstrapped.bootstrap as bs\n",
    "import bootstrapped.compare_functions as bs_compare\n",
    "import bootstrapped.stats_functions as bs_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba2201b1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_central_tendency_and_error(id_central, id_error, sample):\n",
    "\n",
    "    try:\n",
    "        id_error = int(id_error)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    if id_central == \"mean\":\n",
    "        central = np.nanmean(sample, axis=1)\n",
    "    elif id_central == \"median\":\n",
    "        central = np.nanmedian(sample, axis=1)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    if isinstance(id_error, int):\n",
    "        low = np.nanpercentile(sample, q=int((100 - id_error) / 2), axis=1)\n",
    "        high = np.nanpercentile(sample, q=int(100 - (100 - id_error) / 2), axis=1)\n",
    "    elif id_error == \"std\":\n",
    "        low = central - np.nanstd(sample, axis=1)\n",
    "        high = central + np.nanstd(sample, axis=1)\n",
    "    elif id_error == \"sem\":\n",
    "        low = central - np.nanstd(sample, axis=1) / np.sqrt(sample.shape[0])\n",
    "        high = central + np.nanstd(sample, axis=1) / np.sqrt(sample.shape[0])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    return central, low, high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f30e0af",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_test(test_id, data1, data2, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Compute tests comparing data1 and data2 with confidence level alpha\n",
    "    :param test_id: (str) refers to what test should be used\n",
    "    :param data1: (np.ndarray) sample 1\n",
    "    :param data2: (np.ndarray) sample 2\n",
    "    :param alpha: (float) confidence level of the test\n",
    "    :return: (bool) if True, the null hypothesis is rejected\n",
    "    \"\"\"\n",
    "    data1 = data1.squeeze()\n",
    "    data2 = data2.squeeze()\n",
    "    n1 = data1.size\n",
    "    n2 = data2.size\n",
    "\n",
    "    # perform Welch t-test\":\n",
    "    _, p = ttest_ind(data1, data2, equal_var=False)\n",
    "    return p < alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3058af0c",
   "metadata": {},
   "source": [
    "This last function was adapted for the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "24227c11",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def perform_test(perf1, perf2, name1, name2, sample_size=20, downsampling_fact=5, confidence_level=0.01):\n",
    "\n",
    "    perf1 = perf1.transpose()\n",
    "    perf2 = perf2.transpose()\n",
    "    nb_datapoints = perf1.shape[1]\n",
    "    nb_steps = perf1.shape[0]\n",
    "\n",
    "    legend = [name1, name2]\n",
    "\n",
    "    # what do you want to plot ?\n",
    "    id_central = 'mean' # \"median\"  # \n",
    "    id_error = 80  # (percentiles), also: 'std', 'sem'\n",
    "\n",
    "    test_id = \"Welch t-test\"  # recommended\n",
    "    \n",
    "    sample1 = perf1[:, np.random.randint(0, nb_datapoints, sample_size)]\n",
    "    sample2 = perf2[:, np.random.randint(0, nb_datapoints, sample_size)]\n",
    "\n",
    "    steps = np.arange(0, nb_steps, downsampling_fact)\n",
    "    sample1 = sample1[steps, :]\n",
    "    sample2 = sample2[steps, :]\n",
    "\n",
    "    # test\n",
    "    sign_diff = np.zeros([len(steps)])\n",
    "    for i in range(len(steps)):\n",
    "        sign_diff[i] = run_test(\n",
    "            test_id, sample1[i, :], sample2[i, :], alpha=confidence_level\n",
    "        )\n",
    "\n",
    "    central1, low1, high1 = compute_central_tendency_and_error(\n",
    "        id_central, id_error, sample1\n",
    "    )\n",
    "    central2, low2, high2 = compute_central_tendency_and_error(\n",
    "        id_central, id_error, sample2\n",
    "    )\n",
    "\n",
    "    # plot\n",
    "    _, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "    lab1 = plt.xlabel(\"training steps\")\n",
    "    lab2 = plt.ylabel(\"performance\")\n",
    "\n",
    "    plt.plot(steps, central1, linewidth=7)\n",
    "    plt.plot(steps, central2, linewidth=7)\n",
    "    plt.fill_between(steps, low1, high1, alpha=0.3)\n",
    "    plt.fill_between(steps, low2, high2, alpha=0.3)\n",
    "    leg = ax.legend(legend, frameon=False)\n",
    "\n",
    "    # plot significative difference as dots\n",
    "    idx = np.argwhere(sign_diff == 1)\n",
    "    y = max(np.nanmax(high1), np.nanmax(high2))\n",
    "    plt.scatter(steps[idx], y * 1.05 * np.ones([idx.size]), s=100, c=\"k\", marker=\"o\")\n",
    "\n",
    "    # style\n",
    "    for line in leg.get_lines():\n",
    "        line.set_linewidth(10.0)\n",
    "    ax.spines[\"top\"].set_linewidth(5)\n",
    "    ax.spines[\"right\"].set_linewidth(5)\n",
    "    ax.spines[\"bottom\"].set_linewidth(5)\n",
    "    ax.spines[\"left\"].set_linewidth(5)\n",
    "\n",
    "    plt.savefig(\n",
    "        f\"./{name1}_{name2}.png\", bbox_extra_artists=(leg, lab1, lab2), bbox_inches=\"tight\", dpi=100\n",
    "    )\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1567ee0",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "As hyper-parameters, you will use:\n",
    "\n",
    "- naive tuning, that is a pair (0.5, 0.5) for the actor and critic learning rates,\n",
    "- the best hyper-parameters you found with the different tuning algorithms you used before.\n",
    "\n",
    "### 1. For each set of hyper-parameters, collect a large dataset of learning curves.\n",
    "\n",
    "We suggest using 150 training episodes.\n",
    "\n",
    "### 2. Perform statistical comparisons\n",
    "\n",
    "- Take two datasets of learning curves obtained with the hyper-parameters sets that you found with different tuning algorithms.\n",
    "- Use the ``` perform_test(...)``` function to compare each possible pair of sets.\n",
    "\n",
    "You should obtain an image for each pair you have tried.\n",
    "In this image, black dots signal the time step where there is a statistically significant difference between two learning curves.\n",
    "\n",
    " ### 3. Conclude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b20c705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_bayesian_opt_mean_steps = np.load(\"./results/stats_bayesian_optimization_mean_steps.npy\", allow_pickle=True)\n",
    "stats_grid_search_mean_steps = np.load(\"./results/stats_grid_search_mean_steps.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce1f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_bayes_opt_mean_steps = sorted(stats_bayesian_opt_mean_steps, key=lambda x: np.mean(x[\"evaluation_metrics\"][\"mean_steps\"]), reverse=False)\n",
    "top_k_grid_search_mean_steps = sorted(stats_grid_search_mean_steps, key=lambda x: np.mean(x[\"evaluation_metrics\"][\"mean_steps\"]), reverse=False)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Bayesian Optimization - Top 1\")\n",
    "print(\"Metrics: \", top_k_bayes_opt_mean_steps[0][\"evaluation_metrics\"]) \n",
    "print(\"Params: \", top_k_bayes_opt_mean_steps[0][\"params\"])\n",
    "\n",
    "print(\"Grid Search - Top 1\")\n",
    "print(\"Metrics: \", top_k_grid_search_mean_steps[0][\"evaluation_metrics\"])\n",
    "print(\"Params: \", top_k_grid_search_mean_steps[0][\"params\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2558339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnalyseStatistique():\n",
    "    @staticmethod\n",
    "    def evaluate_actor_critic(best_params: dict, nb_episodes: int = 150, n_iter=15):\n",
    "        \n",
    "        best_alpha_critic = best_params[\"alpha_critic\"]\n",
    "        best_alpha_actor = best_params[\"alpha_actor\"]\n",
    "        \n",
    "        results = {}\n",
    "\n",
    "        for i, (alpha_critic, alpha_actor) in enumerate(zip(best_alpha_critic, best_alpha_actor)):\n",
    "\n",
    "            print(f\"Running actor-critic with alpha_critic: {alpha_critic}, alpha_actor: {alpha_actor}\")\n",
    "            \n",
    "\n",
    "            env_v_list, env_time_list, env_mean_reward = [], [], []\n",
    "            for env_i in envs:\n",
    "                all_v_list, all_time_list, all_mean_reward = [], [], []\n",
    "                for _ in range(n_iter):\n",
    "                    _, _, v_list, time_list, reward_list = actor_critic(\n",
    "                        env_i, alpha_critic=alpha_critic, alpha_actor=alpha_actor, nb_episodes=nb_episodes, timeout=config.timeout, render=False\n",
    "                    )\n",
    "                    all_v_list.append(v_list)\n",
    "                    all_time_list.append(time_list)\n",
    "                    all_mean_reward.append(reward_list)\n",
    "\n",
    "                env_v_list.append(np.mean(all_v_list, axis=0))\n",
    "                env_time_list.append(np.mean(all_time_list, axis=0))\n",
    "                env_mean_reward.append(np.mean(all_mean_reward, axis=0))\n",
    "\n",
    "            \n",
    "            all_v_list = np.mean(env_v_list, axis=0)\n",
    "            all_time_list = np.mean(env_time_list, axis=0)\n",
    "            all_mean_reward = np.mean(env_mean_reward, axis=0)\n",
    "\n",
    "            results[i] = {\n",
    "                \"statistical_lists\": {\n",
    "                    \"all_v_list\": np.array(env_v_list),\n",
    "                    \"all_time_list\": np.array(env_time_list),\n",
    "                    \"all_mean_reward\": np.array(env_mean_reward),\n",
    "                },\n",
    "                \"lists\": {\n",
    "                    \"v_list\": all_v_list,\n",
    "                    \"time_list\": all_time_list,\n",
    "                    \"mean_reward\": all_mean_reward,\n",
    "                },\n",
    "                \"name\" : f\"alpha_critic: {np.round(alpha_critic,3)}, alpha_actor: {np.round(alpha_actor,3)}\",\n",
    "                \"id\": i\n",
    "            }\n",
    "\n",
    "        return results\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_statistical_performance(results:dict):\n",
    "        names = {\n",
    "            0: \"Default\",\n",
    "            1: \"Bayesian Optimization\",\n",
    "            2: \"Grid Search\",\n",
    "        }\n",
    "        \n",
    "        for i in range(len(results)-1):\n",
    "            for j in range(i+1, len(results)):\n",
    "                name, name2 = names[i], names[j]\n",
    "\n",
    "                print(f\"Comparing {name} and {name2}\",i,j)\n",
    "                perform_test(results[i]['statistical_lists'][\"all_time_list\"], results[j]['statistical_lists'][\"all_time_list\"], name, name2+\" steps\")\n",
    "                perform_test(results[i]['statistical_lists'][\"all_mean_reward\"], results[j]['statistical_lists'][\"all_mean_reward\"], name, name2+\" reward\")\n",
    "                perform_test(results[i]['statistical_lists'][\"all_v_list\"], results[j]['statistical_lists'][\"all_v_list\"], name, name2+\" value norm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "33df1bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running actor-critic with alpha_critic: 0.5, alpha_actor: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Cours-Sorbonne/M2/UE_DEEP/.venv/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.gamma to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.gamma` for environment variables or `env.get_wrapper_attr('gamma')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running actor-critic with alpha_critic: 0.804400103698065, alpha_actor: 0.9882472809398611\n",
      "Running actor-critic with alpha_critic: 0.8436842105263158, alpha_actor: 1.0\n"
     ]
    }
   ],
   "source": [
    "best_params = {\n",
    "    \"alpha_critic\": [\n",
    "        0.5, \n",
    "        np.float64(top_k_bayes_opt_mean_steps[0][\"params\"][\"alpha_critic\"]), \n",
    "        np.float64(top_k_grid_search_mean_steps[0][\"params\"][\"alpha_critic\"])\n",
    "    ],\n",
    "    \"alpha_actor\": [\n",
    "        0.5, \n",
    "        np.float64(top_k_bayes_opt_mean_steps[0][\"params\"][\"alpha_actor\"]), \n",
    "        np.float64(top_k_grid_search_mean_steps[0][\"params\"][\"alpha_actor\"])\n",
    "    ],\n",
    "}\n",
    "\n",
    "results = AnalyseStatistique.evaluate_actor_critic(best_params, nb_episodes=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bbe42deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid command name \"140189973536064process_stream_events\"\n",
      "    while executing\n",
      "\"140189973536064process_stream_events\"\n",
      "    (\"after\" script)\n",
      "can't invoke \"event\" command: application has been destroyed\n",
      "    while executing\n",
      "\"event generate $w <<ThemeChanged>>\"\n",
      "    (procedure \"ttk::ThemeChanged\" line 6)\n",
      "    invoked from within\n",
      "\"ttk::ThemeChanged\"\n"
     ]
    }
   ],
   "source": [
    "Plotter.compare_plots(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9503b277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Default and Bayesian Optimization 0 1\n",
      "Comparing Default and Grid Search 0 2\n",
      "Comparing Bayesian Optimization and Grid Search 1 2\n"
     ]
    }
   ],
   "source": [
    "AnalyseStatistique.compare_statistical_performance(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fae786",
   "metadata": {},
   "source": [
    "# Lab report\n",
    "\n",
    "Your report should contain:\n",
    "- your source code (probably this notebook), do not forget to put your names on top of the notebook,\n",
    "- in a separate pdf file with your names in the name of the file:\n",
    "    + a detailed enough description of the choices you have made: the parameters you have set, the libraries you have used, etc.,\n",
    "    + the heatmaps obtained using the hyper-parameters tuning algorithms that you have used,\n",
    "    + the figures resulting from performing Welch's T-test using the best hyper-parameters from the above approaches,\n",
    "    + your conclusion from these experiments.\n",
    "\n",
    "Beyond the elements required in this report, any additional studies will be rewarded.\n",
    "For instance, you can try using a Q-function as critic, using random search as hyper-parameters tuning algorithm,\n",
    "using more challenging environments, etc."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
